{"cells":[{"cell_type":"markdown","source":["#Univariate Feature Selection"],"metadata":{}},{"cell_type":"markdown","source":["## Table of Contents\n1. Introduction to univariate feature selection \n1. The `SelectKBest` and `SelectPercentile` transformer classes"],"metadata":{}},{"cell_type":"markdown","source":["##1. Introduction to univariate feature selection"],"metadata":{}},{"cell_type":"markdown","source":["Univariate feature selection techniques evaluate a single feature by applying a scoring function to each feature (possibly in relation to the target feature), and choose features based on their rank with regard to the function.\n\nSee _Feature selection_ at Wikipedia for additional details.\n- https://en.wikipedia.org/wiki/Feature_selection"],"metadata":{}},{"cell_type":"markdown","source":["To perform univariate feature selection, you need to:\n\n- define the __number of features__ that you want to keep.\n- select the __scoring function__ that will evaluate the relationship between the variables."],"metadata":{}},{"cell_type":"markdown","source":["##2. The `SelectKBest` and `SelectPercentile` transformer classes"],"metadata":{}},{"cell_type":"markdown","source":["The `Scikit-learn` library provides classes to use with a suite of different statistical tests in order to select a specific number (percentage) of features.\n\nFor the __number of features__, you can define it through:\n- The `SelectKBest` transformer class. Selects the k best features.\n- The `SelectPercentile` transformer class. Selects the best features into the percentile that you define.\n\nRegarding the __scoring functions__, you'll have different functions for classification and regression problems. \n\nFor classification problem, the two scoring functions are mostly used:\n- `f_classif`. Based on analysis of variance (ANOVA).\n- `mutual_info_classif`. Based on mutual information.\n\nFor regression problem, the two scoring functions are mostly used:\n- `f_regression`. Based on correlation between label and feature.\n- `mutual_info_regression` Based on mutual information."],"metadata":{}},{"cell_type":"markdown","source":["###2.1 `SelectKBest`"],"metadata":{}},{"cell_type":"markdown","source":["The class `SelectBest` in the `sklearn.feature_selection` module can be used to remove all but the k highest scoring features. For instance, we can perform an analysis of variance (ANOVA) F-test to the iris dataset to retrieve only the two best features as follows:"],"metadata":{}},{"cell_type":"markdown","source":["Load the libraries."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["Load iris data."],"metadata":{}},{"cell_type":"code","source":["iris = pd.read_csv('/dbfs/mnt/datalab-datasets/file-samples/iris.csv')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Create features `X` and target `y`."],"metadata":{}},{"cell_type":"code","source":["X = iris.values[:,0:-1]\ny = iris.values[:,-1]"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Create an intance of `SelectKBest` class and store it in an object `selector`. Set the scoring function as `f_classif` and `k=2` to select two features with the highest ANOVA F-Values."],"metadata":{}},{"cell_type":"code","source":["selector = SelectKBest(f_classif, k=2)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["The `fit` method computes the ANOVA F-values from `(X, y)`."],"metadata":{}},{"cell_type":"code","source":["selector.fit(X, y)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["After calling `fit` method, the F-values of features are stored in the `scores_` attribute and the p-values are stored in the `pvalues_` attribute."],"metadata":{}},{"cell_type":"code","source":["selector.scores_, selector.pvalues_"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["The `get_support()` method of the object returns a boolean array which indicates whether a corresponding feature is selected for retention or not."],"metadata":{}},{"cell_type":"code","source":["selector.get_support()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Call the `transform` method to reduce `X` to the selected features. Store the selected features in a new variable `X_kbest`."],"metadata":{}},{"cell_type":"code","source":["X_kbest = selector.transform(X)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Use the `shape` method of the two arrays `X` and `X_kbest` to display the number of features before and after feature selection."],"metadata":{}},{"cell_type":"code","source":["print('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_kbest.shape[1])"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Display names of the selected features according to the boolean array returned by the `get_support()` method."],"metadata":{}},{"cell_type":"code","source":["names = iris.columns.values[0:-1]\nprint('Selected Features:')\nfor i in range(X.shape[1]):\n  if selector.get_support()[i] == True:\n    print(names[i])"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["This section introduces using a transformer class `SelectKBest` to select features according to the k highest scores, as well as displaying the number and names of selected features."],"metadata":{}},{"cell_type":"markdown","source":["###2.2 `SelectPercentile`"],"metadata":{}},{"cell_type":"markdown","source":["The class `SelectPercentile` in the `sklearn.feature_selection` module can be used to remove all but a user-specified highest scoring percentage of features. For instance, we can estimate mutual information (MI) from the iris dataset to retrieve only the top 50% best features as follows.\n\n- _About mutual information(MI): mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency._"],"metadata":{}},{"cell_type":"markdown","source":["Load the libraries."],"metadata":{}},{"cell_type":"code","source":["from sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import mutual_info_classif"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Create an intance of `SelectPercentile` class and store it in an object `selector_mi`. Set the scoring function as `mutual_info_classif` and `percentile=50` to select features with the top 50% highest mutual information (MI) estimation."],"metadata":{}},{"cell_type":"code","source":["selector_mi = SelectPercentile(mutual_info_classif, percentile=50)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["The `fit` method computes the estimated mutual information between each feature and the target."],"metadata":{}},{"cell_type":"code","source":["selector_mi.fit(X, y)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["After calling `fit` method, the estimated MI scores of features are stored in the `scores_` attribute."],"metadata":{}},{"cell_type":"code","source":["selector_mi.scores_"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["The `get_support()` method of the object returns a boolean array which indicates whether a corresponding feature is selected for retention or not."],"metadata":{}},{"cell_type":"code","source":["selector_mi.get_support()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Call the `transform` method to reduce `X` to the selected features. Store the selected features in a new variable `X_pct`."],"metadata":{}},{"cell_type":"code","source":["X_pct = selector_mi.transform(X)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["Display the number of features before and after feature selection."],"metadata":{}},{"cell_type":"code","source":["print('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_pct.shape[1])"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["Display names of the selected features."],"metadata":{}},{"cell_type":"code","source":["print('Selected Features:')\nfor i in range(X.shape[1]):\n  if selector_mi.get_support()[i] == True:\n    print(names[i])"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["Note: The selected features may contain multicollinearity since the univariate feature selection methods do not remove multicollinearity. Methods of dealing with multicollinearity in data won't be illustrated in this notebook."],"metadata":{}},{"cell_type":"markdown","source":["This section introduces using a transformer classes `SelectPercentile` to select features according to a percentile of the highest scores, as well as displaying the number and names of the selected features."],"metadata":{}}],"metadata":{"name":"2. Univariate Feature Selection","notebookId":478847},"nbformat":4,"nbformat_minor":0}